

Prompt:
    写一个高质量Prompt，把任务要求、格式、关键定义以及从 train.json 中精心挑选的几个典型样本（few-shot examples）都放进去，然后直接让 LLM 处理测试数据。

    挑选Few-shot样本: 从 train.json 中挑出 5-10 个最有代表性的例子放入 Prompt。一定要包含：
    简单直接的例子 (id: 396)
    多标签的例子 (id: 1778)
    多四元组的例子 (id: 5219)
    NULL 目标的例子 (id: 2825)
    non-hate 但有争议的例子 (id: 6458)
    包含网络黑话的例子 (id: 6159)
    {
    "instruction": "你是一个中文仇恨言论识别专家。请根据输入文本，抽取出所有仇恨言论四元组（评论对象 | 论点 | 目标群体 | 是否仇恨）。严格按照指定格式输出。",
    "input": "没爹的黑孩到处扔",
    "output": "没爹的黑孩 | 到处扔 | Racism | hate [END]"
    }


评价指标：
    硬匹配 (Hard Match): 这是最严格的部分。Target, Argument, Targeted Group, Hateful 每一个字符都必须完全一样。

    启示1：分类标签必须完美。 
    Targeted Group 和 Hateful 这两个字段没有任何容错空间。
    "Racism" 和 "racism" 就是0分。"Sexism, Racism" 和 "Racism, Sexism" 也可能因为顺序不同而判错。
    这说明标签的标准化和格式的精确控制至关重要。
    启示2：抽取片段也要求高。 
    Target 和 Argument 的抽取也必须和标准答案一字不差。
    
    软匹配 (Soft Match): 这是给你“捞分”的部分。

    启示3：分类标签依然要求完美。 
    注意，软匹配只放宽了对 Target 和 Argument 的要求，Targeted Group 和 Hateful 仍然要求完全一致。
    启示4：核心意思对即可。 
    Target 和 Argument 的字符相似度超过50%即可得分。这意味着模型抽取的片段不需要和答案完全一样，只要抓住了核心词汇，就很有可能得分。例如，原文是“没听说也没看见哪个女生和黑人谈恋爱”，标准答案的Argument是“没听说也没看见哪个女生和黑人谈恋爱”，你的模型如果抽取出“没见过女生和黑人谈恋爱”，虽然硬匹配失败，但软匹配大概率成功。





### **任务：中文社交媒体细粒度仇恨言论识别**

你是一个顶级的中文社交媒体内容审查专家，拥有社会学、语言学和网络文化背景。你的任务是精确地分析给定的文本，抽取出其中所有构成或不构成仇恨言论的观点，并严格按照指定的四元组格式输出。

---

### **第一部分：核心规则与定义**

**1. 输出格式 (必须严格遵守):**
- 每个观点都必须格式化为一个四元组：`评论对象 | 论点 | 目标群体 | 是否仇恨`
- 四元组的每个元素之间用 ` | ` (空格英文半角竖线空格) 分隔。
- 每个四元组必须以 ` [END]` (空格[END]) 结尾。
- 如果一条文本包含多个独立的观点，不同的四元组之间用 ` [SEP] ` (空格[SEP]空格) 分隔。

**2. 四元组字段定义:**
- **评论对象 (Target):** 评论对象，观点所直接指向的人物、群体或实体。
    - **风格对齐:** 抽取简洁的核心短语，严格重视于原文，必须是原文包含的
    - **特殊情况:** 如果言论是泛指或隐含的，没有明确的评论对象，则设为 `NULL`。
- **论点 (Argument):** 对“评论对象”的核心评价、行为描述或观点。
    - **风格对齐:** 必须是从原文中抽取的、最能代表核心观点的简洁短语或子句，不要自创，标点符号也要与原文保持一致。
- **目标群体 (Targeted Group):** 对“评论对象-论点”组合所攻击的群体进行的分类。必须是以下标签之一或多个的组合：
    - `Racism`: 种族或民族歧视。
    - `Region`: 地域歧视。
    - `Sexism`: 性别歧视。
    - `LGBTQ`: 对性少数群体的歧视。
    - `others`: 其他类型的歧视（如针对残障、疾病、宗教、职业等）。
    - `non-hate`: 不构成对特定群体的仇恨。
- **是否仇恨 (Hateful):** 对该观点是否构成仇恨言论的判断。
    - `hate`: 构成仇恨言论。
    - `non-hate`: 不构成仇恨言论（包括一般性人身攻击或无害评论）。

**3. 黄金规则 (为硬匹配优化):**
- **分类标签必须完美:** `Targeted Group` 和 `Hateful` 的标签必须与上述列表完全一致，包括大小写。
- **多标签必须排序:** 如果一个观点涉及多个`目标群体`，标签**必须按照英文字母顺序排序**，并用 `, ` (逗号+空格) 分隔。例如：应为 `Racism, Sexism`，而不是 `Sexism, Racism`。

**4. 常见网络黑话/隐语提示:**
- 为了帮助你更好地理解，以下是一些常见的隐语及其通常指向的歧视类别：
- Racism：
  - 黑蛆、黑皮、泥革、墨人、尼葛 → Racism
  - 黑人器大活好、非洲boy、黑人传艾滋 → Racism
  - 倭狗、日杂、韩畜 → Racism / Other
- Sexism：
  - 女拳、钕拳、小仙女、小酰钕、普信女、eg → Sexism
  - 母狗、母坦克、母零 → Sexism
  - 公、郭楠、龟男、直男癌 → Sexism
- LGBTQ：
  - 基佬、gay、腐癌、txl、同性癖、打拳、虹拳、跪虫族 → LGBTQ
- Region：
  - 东北人、南蛮、南满、北狄、冀东辽西 → Region
  - 河南、河北、安徽、广东、江苏等地名 + 嘲讽、污蔑语言 → Region
---

### **第二部分：典型样本学习 (Few-shot Examples)**

下面是几个经过精心挑选的例子，请仔细学习其内在逻辑。

**【样例1：简单直接的例子】**
- **输入:** `没爹的黑孩到处扔`
- **输出:** `没爹的黑孩 | 到处扔 | Racism | hate [END]`
- **//分析:** 直接针对特定种族（黑孩）进行恶意攻击，是明确的种族主义仇恨言论。

**【样例2：多标签的例子】**
- **输入:** `我靠真能下得去手啊，我去主校区的时候也见过几个又黑又肥又丑看不清脸，可能是我的学校太好了女生都挺保守的，没听说也没看见哪个女生和黑人谈恋爱`
- **输出:** `黑人 | 没听说也没看见哪个女生和黑人谈恋爱 | Racism, Sexism | hate [END]`
- **//分析:** 观点核心是“女生不应和黑人谈恋爱”。这既包含了对黑人群体的负面刻板印象（`Racism`），也包含了对女性择偶自由的物化和限制（`Sexism`）。注意标签已按字母排序。

**【样例3：多四元组的例子】**
- **输入:** `想反女权还得看女权，舔狗龟男理中客们不是男人能打醒的`
- **输出:** `女权 | 想反女权还得看女权 | Sexism | hate [SEP] 舔狗龟男理中客们 | 不是男人能打醒的 | Sexism | hate [END]`
- **//分析:** 文本包含两个独立的攻击点。第一个指向“女权”，第二个指向“舔狗龟男理中客们”，都与性别议题相关。因此需要拆分为两个四元组，用 `[SEP]` 分隔。

**【样例4：NULL目标的例子】**
- **输入:** `都弯了180度所以直了`
- **输出:** `NULL | 弯了180度所以直了 | LGBTQ | hate [END]`
- **//分析:** 这句话是网络上对同性恋群体的暗讽，没有明确说出评论对象，但仇恨指向非常清晰。因此“评论对象”为 `NULL`。

**【样例5：non-hate但有争议的例子】**
- **输入:** `无风不起浪，我就被河南人骗了几回了，没得洗`
- **输出:** `河南人 | 骗了几回了 | non-hate | non-hate [END]`
- **//分析:** 这是此任务标注标准的一个微妙之处。虽然内容是对特定地域人群的负面概括，但因为它基于“个人经历”的叙述，并未上升到鼓吹普遍性歧视或攻击的程度，因此被标注为 `non-hate`。你需要学习这种边界。

**【样例6：包含网络黑话的例子】**
- **输入:** `就算她不拿刀你就能打得过180斤起步的母坦克?`
- **输出:** `母坦克 | 180斤起步 | Sexism | hate [END]`
- **//分析:** 这里的核心攻击点是蔑称“母坦克”，这是一个基于体重的、对女性的侮辱性黑话，属于性别歧视 `Sexism`。

---

### **第三部分：开始任务**

现在，你已经掌握了所有规则和模式。请处理以下新的输入文本，并只返回严格符合格式要求的四元组输出，不要添加任何额外的解释或评论。
**警告：输出格式的绝对精确性**
- 你的输出将用于机器自动评测，任何格式错误，即使是单个空格、大小写或标点符号的偏差，都将导致评测失败。
- 请像机器一样精确地输出，不要添加任何与格式无关的、解释性的文字。你的整个回答应该只有四元组本身。'''




### 层面一：数据为王 (Data-Centric) - 性价比最高

**在现有模型基础上，这是提升效果最明显、最重要的一步。** 模型的上限取决于它所学习的数据。


* **步骤二：分析错误类型**
    * 对照着`test.json`的原文和您期望的正确答案，逐条分析这些失败的例子。问自己几个问题：
        
        * 模型是不是在处理包含**多个`[SEP]`的复杂句子**时表现不佳？
        * 模型是不是对我们之前讨论过的“**有争议的non-hate**”边界把握不准？

* **步骤三：“靶向”数据增强**根据分析，**手动编写5-10条新的、高质量的训练数据**，专门针对模型出错的类型。
    * **示例：如发现模型不认识“普信男”，就在您的`train.json`中加入几条新的、包含“普信男”的样本，并给出正确的标注。
        ```json
        {
          "id": 9001,
          "content": "那个普信男还在那里自我感觉良好，笑死我了。",
          "output": "那个普信男 | 还在那里自我感觉良好 | Sexism | hate [END]"
        },
        {
          "id": 9002,
          "content": "别那么普通却又那么自信，普信男收收味。",
          "output": "普信男 | 收收味 | Sexism | hate [END]"
        }
        ```
    * 将这些新数据加入到您的`train.json`中，然后**用这份“增强版”的数据集重新进行一次微调**。这个过程就像是给模型“补课”，效果立竿见影。

#### 3. 超参数调优 (Hyperparameter Tuning)

微调过程中的参数会影响最终效果。
* **学习率 (learning\_rate)**：`2e-4` 是一个不错的起点。您可以尝试更小的值，比如 `1e-4`, `5e-5`，让模型学习得更“细致”。
* **训练轮数 (num\_train\_epochs)**：我们用了3轮。您可以试试2轮或4轮。在训练过程中密切关注 `eval_loss`（验证集损失），如果它在第2轮后就开始上升，说明模型可能过拟合了，应该提前停止。
* **LoRA Rank (r)**：我们用了 `r=16`。您可以尝试增大它，比如 `32` 或 `64`。更高的Rank意味着LoRA适配器有更多的参数来学习任务，可能效果更好，但同时也增加了过拟合的风险。


#### 4. 模型融合 (Ensemble)

这是在各类比赛中屡试不爽的“大杀器”。
* **方案一：不同种子训练**
    * 使用完全相同的代码和数据，但在`TrainingArguments`中设置不同的随机种子（`seed=42`, `seed=2024`, `seed=101`）。
    * 这样您会得到3个略有不同的微调模型。
    * 在推理时，让3个模型分别对同一条数据进行预测。对于最终结果，采取“**三局两胜**”的投票原则。例如，如果两个模型都输出了`A | B | C | D [END]`，而第三个模型输出不同，那就采纳前两个的结果。这能极大提升模型的稳定性和准确率。
* **方案二：不同模型融合**
    * 用同样的数据，再微调一个完全不同的模型，比如 `ChatGLM3-6B`。
    * 将Qwen的预测结果和ChatGLM的预测结果进行投票融合。

#### 5. 增强后处理逻辑

现在的后处理脚本主要是修复格式。您可以根据错误分析的结果，为它增加一些基于规则的“智能”修复。
在错误分析中发现，模型总是将包含“讨厌黑人”的句子错误地标为`non-hate`。您可以在后处理脚本中加入一条规则：
    ```python
    # 伪代码
    if "讨厌" in argument and "黑人" in target and hateful == "non-hate":
        # 这很可能是个错误，标记出来进行人工检查，或者强制修正为 'hate'
        hateful = "hate" 
    ```
    这种方法需要小心使用，但对于修正模型系统性的偏见非常有效。

后处理：
四元组结构修复（防止缺字段、多字段、多空格）；

目标群体字段标准化：如 lowercase 转大写、拼写校正（"racism" → "Racism"）；

自定义词典映射 + 模板强纠正：基于你构造的黑话词典自动判断；

Argument 和 Target 的截断合并修复：补全被切断的片段；
风格对齐：匹配短语还是句？
标点符号：
格式：
后处理校正 (Post-processing Correction) (性价比极高!)
    实现： 编写一个校正函数，用正则表达式或字符串替换来执行以下操作：
    标签校正： 强制将模型输出的Targeted Group和Hateful字段转为标准形式（例如，"lgbtq" -> "LGBTQ", "sexism" -> "Sexism"）。
